{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯/naive bayes\n",
    "训练集独立同分布时分类效果较好。\n",
    "\n",
    "优点：在数据集较少的情况下仍然有效，可以处理多类别的问题。\n",
    "\n",
    "缺点：对于输入数据的准备方式较为敏感。\n",
    "\n",
    "适用数据类型：标称型数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    \"\"\"获得整个数据集中的文档中所有不重复的单词集合\"\"\"\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #符号‘|’表示求并集。union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"将文档转化为词向量，vocabList为所有单词组成的单词集合，inputSet为某个文档\n",
    "    如果文档中出现某个人word，则vocabList中该word所对应的位置i取值为1，否则为0\n",
    "    最终获得的词向量形式为（0,1,...,1,0,1...)\"\"\"\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['take', 'park', 'help', 'him', 'has', 'is', 'mr', 'posting', 'not', 'stupid', 'cute', 'dog', 'flea', 'stop', 'I', 'worthless', 'garbage', 'ate', 'so', 'dalmation', 'steak', 'maybe', 'love', 'my', 'how', 'please', 'licks', 'buying', 'quit', 'to', 'food', 'problems']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试上述定义函数\n",
    "listOposts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOposts)\n",
    "print(myVocabList),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(setOfWords2Vec(myVocabList, listOposts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(setOfWords2Vec(myVocabList, listOposts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    \"\"\"trainMatrix:文档矩阵\n",
    "    trainCategory：文档类别标签向量，类似[1,1,0,0,1,...]\n",
    "    返回每个类别的条件概率\n",
    "    p0Vect：类别0的情况下，词向量中每个词的条件概率列表\"\"\"\n",
    "    numTrainDocs = len(trainMatrix) #训练集中文档纵数量\n",
    "    numWords = len(trainMatrix[0]) #文档中的单词数量\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)  #侮辱性文档占比，1-pAbusive为正常文档占比\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() ，np.ones(3)-->[1,1,1],啦普拉斯平滑\n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0,初始化分母值\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()，防止下溢出，即太多很小的数相乘，使结果趋进0\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p1Num += trainMatrix[i] ，array列表相加机制\n",
    "array([1,2,3]) + array([2,1,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#测试\n",
    "trainMat = []\n",
    "for postinDoc in listOposts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0v,p1v,pAb = trainNB0(trainMat,listClasses)\n",
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.25809654, -3.25809654, -2.56494936, -2.15948425, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -3.25809654, -3.25809654, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -3.25809654, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -3.25809654, -2.56494936, -1.87180218, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -3.25809654, -3.25809654, -2.56494936,\n",
       "       -3.25809654, -2.56494936])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.35137526, -2.35137526, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -2.35137526, -2.35137526, -1.65822808,\n",
       "       -3.04452244, -1.94591015, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -1.94591015, -2.35137526, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -3.04452244, -2.35137526, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -2.35137526, -2.35137526, -2.35137526,\n",
       "       -2.35137526, -3.04452244])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"vec2Classify：要测试的输入向量\n",
    "    \"\"\"\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #元素点乘-->[1,2,3]*[1,2,3] = [1,4,9]\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据准备工作--->切分文本\n",
    "mySent = 'This book is the best book on python or M.L. I have ever laid eyes upon.'\n",
    "#使用正则表达式切分\n",
    "import re\n",
    "regEx = re.compile(r'\\W+')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "print(listOfTokens),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去掉空字符串和长度小于等于1的字符串\n",
    "print([tok for tok in listOfTokens if len(tok)>1]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将字符串全部转化乘小写或大写\n",
    "print([tok.lower() for tok in listOfTokens if len(tok)>1]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W+', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "    \n",
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        spamPath = '/home/wl/文档/machinelearninginaction/Ch04/email/spam/%d.txt' % i\n",
    "        wordList = textParse(open(spamPath, 'r',encoding='utf8').read())\n",
    "        docList.append(wordList) #构建数据集矩阵\n",
    "        fullText.extend(wordList) #构造所有邮件中所包含的单词列表，用于制作词向量\n",
    "        classList.append(1)  #将spam 文件夹里的邮件设置标签为1\n",
    "        hamPath = '/home/wl/文档/machinelearninginaction/Ch04/email/ham/%d.txt' % i\n",
    "        wordList = textParse(open(hamPath,'r',encoding='utf8').read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)  #将ham 文件夹里的邮件设置标签为0\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    trainingSet = range(50); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet))) #生成一个随机实数\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"classification error\",docList[docIndex])\n",
    "    print('the error rate is: ',float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 884: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-dea3e8b50c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspamTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-cc62b56ece0f>\u001b[0m in \u001b[0;36mspamTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclassList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#将spam 文件夹里的邮件设置标签为1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mhamPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/wl/文档/machinelearninginaction/Ch04/email/ham/%d.txt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mwordList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhamPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mdocList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mfullText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wl/anaconda3/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 884: invalid start byte"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 使用朴素贝叶斯分类器型从个人广告中获取区域倾向\n",
    "RSS程序库的使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-fdcaf9ec857d>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-fdcaf9ec857d>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    URL = https://pypi.python.org/pypi/feedparser#downloads\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#安装feedparse库\n",
    "\n",
    "#下载安装包，复制下述网址下载\n",
    "URL = https://pypi.python.org/pypi/feedparser#downloads\n",
    "\n",
    "#解压安装，先进入解压后的文件加，终端输入：\n",
    "cd '/home/wl/feedparser-5.2.1'\n",
    "\n",
    "#输入以下代码安装\n",
    "python setup.py install\n",
    "\n",
    "#大功告成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用Craigslist上的个人广告\n",
    "import feedparser\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny['entries']\n",
    "len(ny['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    \"\"\"词频排序\n",
    "    vocabList：所有词的集合\n",
    "    fullText：某条广告的全部内容\"\"\"\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    #将词频从高到低进行排序，取前30个高频词\n",
    "    #iteritems() --> items()\n",
    "    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedFreq[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def localWords(feed1,feed0):\n",
    "    \"\"\"RSS源分类器函数\n",
    "    参数为两个不同的RSS源，RSS需要以参数形式传入，即从函数外导入，原因在于RSS源会随着时间而改变。\"\"\"\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    #python3中range不返回数组对象，而是返回range对象 ,因此需要在range前加list\n",
    "    trainingSet = list(range(2*minLen)); \n",
    "    testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    return vocabList,p0V,p1V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25\n"
     ]
    }
   ],
   "source": [
    "#测试函数\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "print(len(ny['entries']),len(sf['entries']))\n",
    "# vocaBlist, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.4\n"
     ]
    }
   ],
   "source": [
    "#多次实验，取结果平均值，这样可以得到错误率的精确估计\n",
    "vocaBlist, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.2\n"
     ]
    }
   ],
   "source": [
    "vocaBlist, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "vocaBlist, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.4\n"
     ]
    }
   ],
   "source": [
    "vocaBlist, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny,sf):\n",
    "    \"\"\"最具表征性的词汇显示函数\"\"\"\n",
    "    import operator\n",
    "    vocabList,p0V,p1V=localWords(ny,sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sortedSF:\n",
    "        print(item[0]),\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print (\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sortedNY:\n",
    "        print(item[0]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.3\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "may\n",
      "around\n",
      "there\n",
      "good\n",
      "movies\n",
      "love\n",
      "seeking\n",
      "hey\n",
      "out\n",
      "fit\n",
      "anyone\n",
      "clean\n",
      "area\n",
      "here\n",
      "only\n",
      "san\n",
      "jose\n",
      "interested\n",
      "work\n",
      "meet\n",
      "hello\n",
      "moved\n",
      "your\n",
      "also\n",
      "going\n",
      "enjoy\n",
      "muse\n",
      "find\n",
      "live\n",
      "games\n",
      "summer\n",
      "times\n",
      "different\n",
      "friendship\n",
      "issues\n",
      "eyes\n",
      "chat\n",
      "other\n",
      "else\n",
      "far\n",
      "maybe\n",
      "host\n",
      "life\n",
      "man\n",
      "trying\n",
      "week\n",
      "sbm\n",
      "kind\n",
      "any\n",
      "woman\n",
      "this\n",
      "healthy\n",
      "hangout\n",
      "apprentice\n",
      "inspirer\n",
      "day\n",
      "cut\n",
      "been\n",
      "dinner\n",
      "bay\n",
      "need\n",
      "how\n",
      "could\n",
      "sun\n",
      "fandoms\n",
      "moblie\n",
      "dating\n",
      "clara\n",
      "non\n",
      "they\n",
      "listening\n",
      "married\n",
      "minded\n",
      "hit\n",
      "parts\n",
      "respectful\n",
      "his\n",
      "betterif\n",
      "then\n",
      "free\n",
      "pretty\n",
      "upfront\n",
      "grew\n",
      "soon\n",
      "usual\n",
      "hikin\n",
      "something\n",
      "music\n",
      "struggle\n",
      "doing\n",
      "riding\n",
      "right\n",
      "either\n",
      "holiday\n",
      "welcome\n",
      "now\n",
      "hang\n",
      "made\n",
      "today\n",
      "cuisines\n",
      "road\n",
      "never\n",
      "13955\n",
      "workout\n",
      "interest\n",
      "twink\n",
      "food\n",
      "call\n",
      "point\n",
      "sexual\n",
      "chance\n",
      "creative\n",
      "lbs\n",
      "pen\n",
      "asian\n",
      "lonely\n",
      "occasion\n",
      "means\n",
      "indian\n",
      "communicate\n",
      "mentor\n",
      "dte\n",
      "style\n",
      "make\n",
      "down\n",
      "contra\n",
      "running\n",
      "smoker\n",
      "three\n",
      "which\n",
      "harry\n",
      "wanna\n",
      "white\n",
      "booty\n",
      "time\n",
      "confident\n",
      "younger\n",
      "possibly\n",
      "milpitas\n",
      "recently\n",
      "lies\n",
      "had\n",
      "pal\n",
      "year\n",
      "drama\n",
      "busy\n",
      "safe\n",
      "story\n",
      "successful\n",
      "things\n",
      "open\n",
      "few\n",
      "before\n",
      "smell\n",
      "niche\n",
      "expectations\n",
      "passionate\n",
      "m4m\n",
      "mareshia\n",
      "posted\n",
      "more\n",
      "worth\n",
      "geeky\n",
      "schedule\n",
      "goodfriend\n",
      "bored\n",
      "shape\n",
      "sno\n",
      "needed\n",
      "swf\n",
      "because\n",
      "slayer\n",
      "age\n",
      "girl\n",
      "topics\n",
      "its\n",
      "long\n",
      "ind\n",
      "adult\n",
      "size\n",
      "santa\n",
      "discreetly\n",
      "social\n",
      "italian\n",
      "miles\n",
      "isn\n",
      "casual\n",
      "post\n",
      "back\n",
      "anr\n",
      "male\n",
      "flexible\n",
      "will\n",
      "potter\n",
      "think\n",
      "ambitious\n",
      "seeks\n",
      "ravemama21\n",
      "active\n",
      "use\n",
      "gets\n",
      "manis\n",
      "races\n",
      "campbell\n",
      "vampire\n",
      "quick\n",
      "sat\n",
      "mimi\n",
      "couple\n",
      "light\n",
      "young\n",
      "weekends\n",
      "anytime\n",
      "ride\n",
      "strictly\n",
      "hours\n",
      "cub\n",
      "smoke\n",
      "abf\n",
      "strings\n",
      "185\n",
      "event\n",
      "fremont\n",
      "truth\n",
      "quite\n",
      "try\n",
      "black\n",
      "weekly\n",
      "please\n",
      "420\n",
      "affectionate\n",
      "doctor\n",
      "etc\n",
      "thomas\n",
      "shot\n",
      "lol\n",
      "trips\n",
      "saying\n",
      "above\n",
      "many\n",
      "buddy\n",
      "treat\n",
      "brown\n",
      "those\n",
      "after\n",
      "pedis\n",
      "view\n",
      "sometime\n",
      "movie\n",
      "friendly\n",
      "nursing\n",
      "sporting\n",
      "kik\n",
      "escape\n",
      "seem\n",
      "while\n",
      "families\n",
      "watch\n",
      "county\n",
      "little\n",
      "sicilian\n",
      "mellow\n",
      "stay\n",
      "reviews\n",
      "thank\n",
      "bear\n",
      "stop\n",
      "come\n",
      "heard\n",
      "same\n",
      "costa\n",
      "since\n",
      "race\n",
      "dependable\n",
      "company\n",
      "sunnyvale\n",
      "write\n",
      "boat\n",
      "guys\n",
      "what\n",
      "sext\n",
      "name\n",
      "from\n",
      "buffy\n",
      "drives\n",
      "guy\n",
      "wants\n",
      "next\n",
      "top\n",
      "people\n",
      "works\n",
      "circle\n",
      "between\n",
      "boring\n",
      "chill\n",
      "hair\n",
      "easy\n",
      "mix\n",
      "trustworthy\n",
      "meeting\n",
      "over\n",
      "physically\n",
      "beautiful\n",
      "based\n",
      "btm\n",
      "really\n",
      "african\n",
      "fat\n",
      "kiss\n",
      "bbw\n",
      "on2\n",
      "beginner\n",
      "fling\n",
      "big\n",
      "ideas\n",
      "connection\n",
      "until\n",
      "whether\n",
      "travel\n",
      "take\n",
      "talking\n",
      "hoping\n",
      "nice\n",
      "cuz\n",
      "exchange\n",
      "fri\n",
      "convo\n",
      "location\n",
      "perfect\n",
      "partciularly\n",
      "well\n",
      "takers\n",
      "city\n",
      "everywhere\n",
      "leave\n",
      "normal\n",
      "morning\n",
      "females\n",
      "sexy\n",
      "anything\n",
      "our\n",
      "midwest\n",
      "american\n",
      "sorry\n",
      "low\n",
      "debt\n",
      "one\n",
      "127997\n",
      "intimate\n",
      "dancing\n",
      "korean\n",
      "expect\n",
      "abusing\n",
      "home\n",
      "intermediate\n",
      "break\n",
      "detail\n",
      "female\n",
      "reply\n",
      "commitment\n",
      "state\n",
      "subject\n",
      "heiress\n",
      "minimum\n",
      "key\n",
      "moves\n",
      "robert\n",
      "dance\n",
      "spontaneo\n",
      "lets\n",
      "sex\n",
      "driving\n",
      "blossom\n",
      "negative\n",
      "everything\n",
      "doesn\n",
      "extremely\n",
      "feedback\n",
      "dick\n",
      "must\n",
      "intelligent\n",
      "partner\n",
      "horror\n",
      "thanks\n",
      "history\n",
      "group\n",
      "mobile\n",
      "tell\n",
      "degrade\n",
      "past\n",
      "drink\n",
      "yourself\n",
      "fascinated\n",
      "college\n",
      "contact\n",
      "myself\n",
      "fashionable\n",
      "skills\n",
      "narrow\n",
      "bar\n",
      "jones\n",
      "awesome\n",
      "nothing\n",
      "rather\n",
      "matter\n",
      "conversation\n",
      "side\n",
      "guitar\n",
      "laughing\n",
      "great\n",
      "bbq\n",
      "every\n",
      "100\n",
      "harujuku\n",
      "friday\n",
      "spirituality\n",
      "tahoe\n",
      "eat\n",
      "platonic\n",
      "submissive\n",
      "oral\n",
      "street\n",
      "desperate\n",
      "aaron\n",
      "student\n",
      "comes\n",
      "decent\n",
      "socially\n",
      "fun\n",
      "way\n",
      "pack\n",
      "pics\n",
      "loves\n",
      "drawing\n",
      "seek\n",
      "hanging\n",
      "night\n",
      "talk\n",
      "columbus\n",
      "degraded\n",
      "given\n",
      "openness\n",
      "stand\n",
      "otherwise\n",
      "york\n",
      "bottom\n",
      "beach\n",
      "views\n",
      "adding\n",
      "don\n",
      "stimulate\n",
      "cemeteries\n",
      "account\n",
      "stores\n",
      "torturing\n",
      "another\n",
      "says\n",
      "fakes\n",
      "thrift\n",
      "won\n",
      "tennis\n",
      "honest\n",
      "car\n",
      "doesnt\n",
      "years\n",
      "however\n",
      "has\n",
      "tried\n",
      "trust\n",
      "likes\n",
      "hamster\n",
      "clothes\n",
      "drinks\n",
      "motel\n",
      "info\n",
      "might\n",
      "hiv\n",
      "random\n",
      "person\n",
      "both\n",
      "hear\n",
      "real\n",
      "personal\n",
      "title\n",
      "reclusive\n",
      "goes\n",
      "dude\n",
      "wanted\n",
      "tonight\n",
      "adventures\n",
      "she\n",
      "panamanian\n",
      "buddhist\n",
      "almost\n",
      "her\n",
      "smh\n",
      "manhattan\n",
      "dressed\n",
      "180\n",
      "them\n",
      "exists\n",
      "public\n",
      "dom\n",
      "natura\n",
      "mature\n",
      "moses\n",
      "favor\n",
      "currently\n",
      "meaningful\n",
      "mind\n",
      "party\n",
      "spend\n",
      "reasonably\n",
      "reading\n",
      "locally\n",
      "number\n",
      "drive\n",
      "line\n",
      "practice\n",
      "salsa\n",
      "pay\n",
      "walked\n",
      "build\n",
      "art\n",
      "older\n",
      "128075\n",
      "6ft\n",
      "heart\n",
      "neighborhood\n",
      "hung\n",
      "photography\n",
      "curious\n",
      "tokyo\n",
      "following\n",
      "was\n",
      "museums\n",
      "hmu\n",
      "playing\n",
      "latino\n",
      "message\n",
      "lives\n",
      "old\n",
      "too\n",
      "april\n",
      "relationship\n",
      "hook\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "talk\n",
      "more\n",
      "drive\n",
      "matter\n",
      "night\n",
      "don\n",
      "day\n",
      "meaningful\n",
      "spend\n",
      "what\n",
      "people\n",
      "over\n",
      "based\n",
      "fling\n",
      "married\n",
      "leave\n",
      "hey\n",
      "out\n",
      "one\n",
      "commitment\n",
      "time\n",
      "thanks\n",
      "friendship\n",
      "college\n",
      "chat\n",
      "rather\n",
      "bored\n",
      "age\n",
      "openness\n",
      "meet\n",
      "another\n",
      "please\n",
      "any\n",
      "420\n",
      "woman\n",
      "honest\n",
      "trust\n",
      "real\n",
      "also\n",
      "number\n",
      "relationship\n",
      "bbw\n",
      "nice\n",
      "location\n",
      "normal\n",
      "anything\n",
      "our\n",
      "food\n",
      "break\n",
      "female\n",
      "minimum\n",
      "moves\n",
      "dance\n",
      "negative\n",
      "doesn\n",
      "anyone\n",
      "myself\n",
      "bar\n",
      "here\n",
      "student\n",
      "will\n",
      "life\n",
      "man\n",
      "degraded\n",
      "work\n",
      "doesnt\n",
      "has\n",
      "trips\n",
      "drinks\n",
      "hiv\n",
      "hear\n",
      "your\n",
      "dude\n",
      "reading\n",
      "practice\n",
      "guy\n",
      "wants\n",
      "need\n",
      "curious\n",
      "old\n",
      "meeting\n",
      "physically\n",
      "beautiful\n",
      "african\n",
      "fat\n",
      "kiss\n",
      "on2\n",
      "beginner\n",
      "big\n",
      "ideas\n",
      "connection\n",
      "until\n",
      "whether\n",
      "parts\n",
      "take\n",
      "cuz\n",
      "exchange\n",
      "convo\n",
      "partciularly\n",
      "well\n",
      "takers\n",
      "city\n",
      "everywhere\n",
      "females\n",
      "sexy\n",
      "american\n",
      "sorry\n",
      "low\n",
      "hang\n",
      "never\n",
      "127997\n",
      "intimate\n",
      "dancing\n",
      "korean\n",
      "expect\n",
      "call\n",
      "point\n",
      "abusing\n",
      "intermediate\n",
      "detail\n",
      "reply\n",
      "state\n",
      "subject\n",
      "find\n",
      "live\n",
      "games\n",
      "key\n",
      "robert\n",
      "lets\n",
      "sex\n",
      "driving\n",
      "blossom\n",
      "white\n",
      "everything\n",
      "summer\n",
      "extremely\n",
      "feedback\n",
      "dick\n",
      "must\n",
      "year\n",
      "there\n",
      "intelligent\n",
      "partner\n",
      "group\n",
      "degrade\n",
      "drink\n",
      "yourself\n",
      "contact\n",
      "open\n",
      "before\n",
      "skills\n",
      "jones\n",
      "clean\n",
      "conversation\n",
      "laughing\n",
      "other\n",
      "great\n",
      "bbq\n",
      "every\n",
      "spirituality\n",
      "eat\n",
      "girl\n",
      "platonic\n",
      "topics\n",
      "social\n",
      "maybe\n",
      "comes\n",
      "decent\n",
      "socially\n",
      "fun\n",
      "host\n",
      "pack\n",
      "pics\n",
      "loves\n",
      "seek\n",
      "given\n",
      "otherwise\n",
      "young\n",
      "york\n",
      "bottom\n",
      "beach\n",
      "week\n",
      "views\n",
      "adding\n",
      "stimulate\n",
      "account\n",
      "torturing\n",
      "fakes\n",
      "this\n",
      "hello\n",
      "won\n",
      "tennis\n",
      "car\n",
      "years\n",
      "however\n",
      "lol\n",
      "tried\n",
      "many\n",
      "likes\n",
      "good\n",
      "buddy\n",
      "movies\n",
      "hamster\n",
      "motel\n",
      "info\n",
      "after\n",
      "personal\n",
      "goes\n",
      "panamanian\n",
      "buddhist\n",
      "almost\n",
      "smh\n",
      "manhattan\n",
      "dressed\n",
      "them\n",
      "public\n",
      "natura\n",
      "little\n",
      "mature\n",
      "moses\n",
      "come\n",
      "mind\n",
      "party\n",
      "company\n",
      "reasonably\n",
      "locally\n",
      "line\n",
      "salsa\n",
      "pay\n",
      "build\n",
      "love\n",
      "older\n",
      "128075\n",
      "following\n",
      "how\n",
      "too\n",
      "hook\n",
      "could\n",
      "sun\n",
      "fandoms\n",
      "seeking\n",
      "btm\n",
      "really\n",
      "moblie\n",
      "dating\n",
      "clara\n",
      "non\n",
      "around\n",
      "they\n",
      "listening\n",
      "minded\n",
      "hit\n",
      "travel\n",
      "talking\n",
      "respectful\n",
      "hoping\n",
      "his\n",
      "betterif\n",
      "then\n",
      "free\n",
      "pretty\n",
      "upfront\n",
      "fri\n",
      "grew\n",
      "perfect\n",
      "soon\n",
      "usual\n",
      "hikin\n",
      "something\n",
      "music\n",
      "morning\n",
      "struggle\n",
      "midwest\n",
      "doing\n",
      "riding\n",
      "right\n",
      "either\n",
      "holiday\n",
      "welcome\n",
      "debt\n",
      "now\n",
      "made\n",
      "today\n",
      "cuisines\n",
      "road\n",
      "13955\n",
      "workout\n",
      "interest\n",
      "twink\n",
      "sexual\n",
      "chance\n",
      "creative\n",
      "lbs\n",
      "pen\n",
      "asian\n",
      "lonely\n",
      "home\n",
      "muse\n",
      "occasion\n",
      "means\n",
      "indian\n",
      "communicate\n",
      "mentor\n",
      "dte\n",
      "style\n",
      "heiress\n",
      "fit\n",
      "make\n",
      "down\n",
      "contra\n",
      "running\n",
      "smoker\n",
      "three\n",
      "spontaneo\n",
      "which\n",
      "harry\n",
      "wanna\n",
      "booty\n",
      "confident\n",
      "younger\n",
      "possibly\n",
      "milpitas\n",
      "recently\n",
      "lies\n",
      "had\n",
      "pal\n",
      "horror\n",
      "times\n",
      "history\n",
      "drama\n",
      "busy\n",
      "mobile\n",
      "safe\n",
      "different\n",
      "tell\n",
      "story\n",
      "past\n",
      "fascinated\n",
      "successful\n",
      "things\n",
      "issues\n",
      "few\n",
      "fashionable\n",
      "smell\n",
      "narrow\n",
      "eyes\n",
      "niche\n",
      "awesome\n",
      "expectations\n",
      "passionate\n",
      "m4m\n",
      "nothing\n",
      "mareshia\n",
      "posted\n",
      "worth\n",
      "side\n",
      "guitar\n",
      "geeky\n",
      "schedule\n",
      "goodfriend\n",
      "100\n",
      "harujuku\n",
      "friday\n",
      "area\n",
      "shape\n",
      "sno\n",
      "needed\n",
      "swf\n",
      "because\n",
      "slayer\n",
      "tahoe\n",
      "its\n",
      "else\n",
      "submissive\n",
      "oral\n",
      "street\n",
      "long\n",
      "ind\n",
      "adult\n",
      "size\n",
      "far\n",
      "santa\n",
      "only\n",
      "discreetly\n",
      "desperate\n",
      "aaron\n",
      "italian\n",
      "miles\n",
      "isn\n",
      "casual\n",
      "post\n",
      "back\n",
      "san\n",
      "anr\n",
      "way\n",
      "male\n",
      "flexible\n",
      "drawing\n",
      "hanging\n",
      "potter\n",
      "think\n",
      "ambitious\n",
      "columbus\n",
      "jose\n",
      "seeks\n",
      "interested\n",
      "ravemama21\n",
      "active\n",
      "use\n",
      "gets\n",
      "stand\n",
      "trying\n",
      "manis\n",
      "races\n",
      "campbell\n",
      "vampire\n",
      "quick\n",
      "sat\n",
      "mimi\n",
      "couple\n",
      "light\n",
      "weekends\n",
      "anytime\n",
      "ride\n",
      "strictly\n",
      "hours\n",
      "cub\n",
      "smoke\n",
      "cemeteries\n",
      "abf\n",
      "strings\n",
      "185\n",
      "sbm\n",
      "stores\n",
      "event\n",
      "fremont\n",
      "truth\n",
      "quite\n",
      "try\n",
      "kind\n",
      "black\n",
      "weekly\n",
      "says\n",
      "thrift\n",
      "affectionate\n",
      "moved\n",
      "doctor\n",
      "etc\n",
      "thomas\n",
      "shot\n",
      "saying\n",
      "above\n",
      "healthy\n",
      "treat\n",
      "clothes\n",
      "brown\n",
      "might\n",
      "random\n",
      "those\n",
      "person\n",
      "both\n",
      "hangout\n",
      "title\n",
      "pedis\n",
      "view\n",
      "reclusive\n",
      "sometime\n",
      "wanted\n",
      "movie\n",
      "tonight\n",
      "adventures\n",
      "she\n",
      "her\n",
      "apprentice\n",
      "friendly\n",
      "nursing\n",
      "sporting\n",
      "kik\n",
      "180\n",
      "escape\n",
      "exists\n",
      "inspirer\n",
      "dom\n",
      "seem\n",
      "while\n",
      "families\n",
      "watch\n",
      "county\n",
      "sicilian\n",
      "mellow\n",
      "stay\n",
      "reviews\n",
      "thank\n",
      "bear\n",
      "favor\n",
      "stop\n",
      "currently\n",
      "heard\n",
      "same\n",
      "costa\n",
      "since\n",
      "race\n",
      "dependable\n",
      "cut\n",
      "sunnyvale\n",
      "write\n",
      "been\n",
      "boat\n",
      "going\n",
      "guys\n",
      "sext\n",
      "name\n",
      "from\n",
      "buffy\n",
      "drives\n",
      "walked\n",
      "may\n",
      "art\n",
      "dinner\n",
      "next\n",
      "top\n",
      "bay\n",
      "6ft\n",
      "works\n",
      "heart\n",
      "enjoy\n",
      "neighborhood\n",
      "hung\n",
      "circle\n",
      "between\n",
      "photography\n",
      "boring\n",
      "chill\n",
      "tokyo\n",
      "was\n",
      "museums\n",
      "hair\n",
      "hmu\n",
      "playing\n",
      "latino\n",
      "message\n",
      "lives\n",
      "easy\n",
      "mix\n",
      "april\n",
      "trustworthy\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny,sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
